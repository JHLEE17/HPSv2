<p align="center"><img src="assets/hps_banner.png"/ width="100%"><br></p>

## HPS v2: Benchmarking Text-to-Image Generative Models

This is the official repository for the paper: Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis. 

<p align="center"><img src="assets/overview.png"/ width="100%"><br></p>

## Overview 
We introduce Human Preference Dataset v2, a large-scale, well-annotated dataset for researches of human preferences on images generated by text-to-image generative models. HPS v2 is a preference prediction model trained on HPD v2, and it exhibits better correlation with human preferences against existing models. We also provide a fair, stable and easy-to-use set of evaluation prompts for text-to-image generative models.

## The HPS v2 benchmark
| Model                 | Animation | Concept-art | Painting | Photo    | DrawBench (for reference) |
| ---------------------| --------- | ----------- | -------- | -------- | --------- |
| GLIDE                 | 0.2334    | 0.2308      | 0.2327   | 0.2450   | 0.2505    |
| LAFITE                | 0.2463    | 0.2438      | 0.2443   | 0.2581   | 0.2523    |
| VQ-Diffusion          | 0.2497    | 0.2470      | 0.2501   | 0.2571   | 0.2544    |
| FuseDream             | 0.2526    | 0.2515      | 0.2513   | 0.2557   | 0.2572    |
| Latent Diffusion      | 0.2573    | 0.2515      | 0.2525   | 0.2697   | 0.2617    |
| CogView2              | 0.2650    | 0.2659      | 0.2633   | 0.2644   | 0.2617    |
| DALL·E mini           | 0.2610    | 0.2556      | 0.2556   | 0.2612   | 0.2634    |
| Versatile Diffusion   | 0.2659    | 0.2628      | 0.2643   | 0.2705   | 0.2677    |
| VQGAN + CLIP          | 0.2644    | 0.2653      | 0.2647   | 0.2612   | 0.2638    |
| DALL·E 2              | 0.2734    | 0.2654      | 0.2668   | 0.2724   | 0.2716    |
| Stable Diffusion v1.4 | 0.2726    | 0.2661      | 0.2666   | 0.2727   | 0.2723    |
| Stable Diffusion v2.0 | 0.2748    | 0.2689      | 0.2686   | 0.2746   | 0.2731    |
| Epic Diffusion        | 0.2757    | 0.2696      | 0.2703   | 0.2749   | 0.2733    |
| Openjourney           | 0.2785    | 0.2718      | 0.2725   | 0.2753   | 0.2744    |
| MajicMix Realistic    | 0.2788    | 0.2719      | 0.2722   | 0.2764   | 0.2747    |
| ChilloutMix           | 0.2792    | 0.2729      | 0.2732   | 0.2761   | 0.2747    |
| DeepFloyd-XL          | 0.2764    | 0.2683      | 0.2686   | 0.2775   | 0.2764    |
| Deliberate            | 0.2813    | 0.2746      | 0.2745   | 0.2762   | 0.2773    |
| Realistic Vision      | 0.2822    | 0.2753      | 0.2756   | 0.2775   | 0.2777    |
| Dreamlike Photoreal 2.0 | 0.2824 | 0.2760      | 0.2759   | 0.2799   | 0.2788    |

## Human Preference Dataset v2
The prompts in our dataset are sourced from DiffusionDB and MSCOCO Captions. Prompts from DiffusionDB are first cleaned by ChatGPT to remove biased function words. Human annotators are tasked to rank images generated by different text-to-image generative models from the same prompt. Totally there are about 798K pairwise comparisons of images for over 430k images and 107k prompts, 645k pairs for training split and 153k pairs for test split.

Image sources of HPD v2:
|  Source | # of images 
| :-----: | :-----: |
| CogView2 | 73697 |
| DALL·E 2 | 101869 | 
| GLIDE (mini) | 400 |
| Stable Diffusion v1.4 | 101869 |
| Stable Diffusion v2.0 | 101869 | 
| LAFITE | 400 | 
| VQ-GAN+CLIP | 400 |
| VQ-Diffusion | 400 |
| FuseDream | 400 |
| COCO Captions | 28272 |

The compressed dataset can be downloaded from [here](??????????).
Once unzipped, you should get a folder with the following structure:
```
HPD
---- train/
-------- {image_id}.jpg
---- test/
-------- {image_id}.jpg
---- train.json
---- test.json
---- benchmark/
-------- benchmark_imgs/
------------ {model_id}/
---------------- {image_id}.jpg
-------- drawbench/
------------ {model_id}/
---------------- {image_id}.jpg
-------- anime.json
-------- concept-art.json
-------- paintings.json
-------- photo.json
-------- drawbench.json
```

The annotation file, `train.json`, is organized as:
```
[
    {
        'human_preference': list[int], # 1 for preference
        'prompt': str,
        'file_path': list[str],
        'user_hash': str,
    },
    ...
]
```

The annotation file, `test.json`, is organized as:
```
[
    {
        'prompt': str,
        'image_path': list[str],
        'rank': list[int], # ranking for image at the same index in image_path
    },
    ...
]
```

The benchmark prompts file, ie. `anime.json` is pure prompts. The corresponding image can be found in the folder of the corresponding model by indexing the prompt.

## Environments

```
# environments
pip install -r requirements.txt 
```

## Evaluation

Evaluating HPS v2's correlation with human preference choices:
|  Model | Acc. on ImageReward test set (%)| Acc. on HPD v2 test set (%)
| :-----: | :-----: |:-----: |
|  [Aesthetic Score Predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor) | 57.4 | 72.6 |
|  [ImageReward](https://github.com/THUDM/ImageReward) | 65.1 | 70.6 |
|  [HPS](https://github.com/tgxs002/align_sd) | 61.2 | 73.1 |
|  [PickScore](https://github.com/yuvalkirstain/PickScore) | 62.9 | 79.8 |
|  Single Human | 65.3 | 78.1 |
|  HPS v2 | 65.7 | 83.3 |



HPS v2 checkpoint can be downloaded from [here](???????????????).
Run the following command to evaluate the HPS v2 model:
```
python evaluate.py --data-type test --data-path /path/to/HPD --image-path /path/to/image_folder --batch-size 10 --checkpoint /path/to/HPSv2.pt

#for example
python evaluate.py --data-type test --data-path data/HPD --image-path data/HPD/test --batch-size 10 --checkpoint ckpt/HPSv2.pt

```
## Train Human Preference Predictor
To train your own human preference predictor, just change the coresponding path in `configs/controller.sh` and run the following command:
```
# if you are running locally
bash configs/HPSv2.sh train ${GPUS} local
# if you are running on slurm
bash configs/HPSv2.sh train ${GPUS} ${quota_type}
```

## Citation